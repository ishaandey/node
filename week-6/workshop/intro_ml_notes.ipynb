{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with anything, we'll start off with our usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML, we'll be using a new package: `sci-kit learn`. It's one of the best in Python, and has all sorts of helpful functions for preparing the data, creating models, and evaluating accuracy.\n",
    "\n",
    "If you don't have this yet installed, use `!pip install sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be taking a look at heart disease data. \n",
    "\n",
    "Every observation (row) is associated with a `target`, a binary indicator for whether or not the patient developed heart disease. The columns each represent some biological metric for that patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/ishaandey/node/master/week-6/workshop/heart.csv'\n",
    "heart = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some numerical conversions\n",
    "heart['target'] = heart['target'].map({1:'heart disease', 0:'no heart disease'})\n",
    "heart['sex'] = heart['sex'].map({1:'male',0:'female'})\n",
    "heart['cp'] = heart['cp'].map({2: 'typical angina', 3: 'atypical angina', 1: 'non-anginal pain', 0: 'asymptomatic'})\n",
    "heart['fbs'] = heart['fbs'].apply(lambda x: np.random.triangular(121, 145, 200) if x==1 else np.random.triangular(68, 100, 119)).astype(int)\n",
    "heart['restecg'] = heart['restecg'].map({0:'normal', 1:'ST-T abnormality', 2:'left ventricular hypertrophy'})\n",
    "heart['exang'] = heart['exang'].map({1:'angina', 0:'no angina'})\n",
    "heart['slope'] = heart['slope'].map({0:'upsloping', 1:'flat', 2:'downsloping'})\n",
    "heart['thal'] = heart['thal'].map({2:'no thalassemia', 1:'fixed defect', 3: 'reversable defect'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the context of which data was obtained is incredibly important. \n",
    "<br>Take a moment to look through the original study documentation: https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In groups, answer the following questions:\n",
    "\n",
    "1. How many hospitals was this data pulled from? Around what year(s)?\n",
    "2. What does the `chol` feature measure?\n",
    "3. In this sample, what proportion of patients had the target condition? *(Use code for this)*\n",
    "4. Lastly, **what type of ML problem is this**? (Supervised vs. Unsupervised? Classification vs. Regression?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms don't have the innate ability to translate English words into mathematical models. We've gotta do that work ourselves.\n",
    "\n",
    "Overall, **our goal is to get *text* into *numbers* **. There's a couple strategies here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Indicator Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator variables are always coded as `1` or `0`, usually corresponding to *condition present* `1` or *condition absent* `0` for that particular observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, heart disease can come in various forms of severity, BUT if we're just interested in **whether or not** the patient has some heart disease, we'll *binarize* that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common use case is with sex. Use the `.map()` function to code females as `1`, and males as `0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning into Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes continuous variables can be better categorized as bins. In other words, we can break up the full range of numbers into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good use of this would be with the fasting blood sugar, using the `pd.cut()` function. We can use the medical criteria of 120 mg/dl as a threshold for high blood sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarizing is great, but happens when we have more than 2 categories? \n",
    "\n",
    "The best way to handle this is to create *new unique column for every category*. \n",
    "<br>We can do this using Pandas' `get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick helper\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\\\"display:inline\\\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side(heart[['cp']].head(5), pd.get_dummies(heart['cp'],prefix='cp').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of some intricacies with the underlying linear algebra, we always drop one of the columns, and consider the others relative to it.\n",
    "\n",
    "That way, if all three columns are `0`, we can assume the chest pain type was asymptomatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side(heart[['cp']].head(5), pd.get_dummies(heart['cp'],prefix='cp', drop_first=True).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this on each categorical column, we can specify a list of features to do it on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols = ['cp','restecg', 'thal', 'slope', 'exang', 'sex', 'fbs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TLDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a lot, I know. But it's incredibly important to understand the *rationale* behind each data transformation. \n",
    "\n",
    "In general, we want to convert text / information => numbers that can represent them. To do so, we can:\n",
    "\n",
    "1. Binarize features\n",
    "2. Bin based on intervals\n",
    "3. Create dummy variables for columns with multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data structures for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step #1: Split the dataset into **feature and target variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different fields use different names, but think of *feature* vars as your (multiple) independent variables, and *target* var as the singular dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side(X.iloc[:5,:9],pd.DataFrame(y).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step #2: Split data into **training and testing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need to do this step? Think back to your SATs in high school. \n",
    "- When practicing for the exam, you were given practice tests that *resembled*, but weren't *exactly* the test you were about to take\n",
    "- If you'd been given the *actual* exam to practice with, your score during the real thing wouldn't mean anything: <br>It *would not* have measured your abilities as a student, just your ability to memorize the particular answers for that particular exam.\n",
    "<br>\n",
    "\n",
    "The same goes with teaching a ML model some patterns in the data. \n",
    "- To allow the model to learn, we pass it *training data*. (Our SAT practice exams)\n",
    "- To determine if the model really did learn, we'll feed in *testing data*. (The actual SAT exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things here:\n",
    "- The `train_test_split` function returns 4 objects, which we're saving all seperately to train and test sets\n",
    "- We're passing in our feature dataframe `X` and target array `y`\n",
    "- We can control what fraction of the total observations should end up in testing set using the `test_size=` parameter. <br>80/20 splits are the convention (80% training, 20% testing).\n",
    "- Because we're randomly assigning which observations end up in which group, we're using the `random_state=` parameter to specify the RNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step #1: Select the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tons of models out there. Some are better at some tasks than others, but there hardly is ever a hard and fast rule that says: \"This is *always* the best algorithm to use\". You'll have to try out different ones and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step #2: Train the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite simply, this is where the model is **learning** the patterns in the data. To do so, we simply **fit** the model onto the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note all those little parameters? We can tune those down the line to improve our model. More on that week 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step #3: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our model did, we simply:\n",
    "1. Predict the results, based on just the feature testing data\n",
    "2. Compare the results against the actual testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = \n",
    "actual = \n",
    "\n",
    "print('Look at first 10 predictions:')\n",
    "print('Predicted: ',predicted[:10])\n",
    "print('Actual:    ',actual[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well obviously, its hard to tell how we did by counting 1s and 0s, so we can use metrics like accuracy to capture the fraction of observations taken correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_tree = \n",
    "print(accuracy_tree)"
   ]
  },
  {
   "source": [
    "## You Try!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Using a **logistic regression classifier**, train and evaluate the model:\n",
    "Which does better?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = \n",
    "actual = \n",
    "\n",
    "print('Look at first 10 predictions:')\n",
    "print('Predicted: ',predicted[:10])\n",
    "print('Actual:    ',actual[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg = \n",
    "print(accuracy_logreg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('node': conda)",
   "language": "python",
   "name": "python37764bitnodeconda078ddf2426c34064a6fe480538b1e0ec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}